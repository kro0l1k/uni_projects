{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[0.4738, 0.5273, 0.4417, 0.4843, 0.6053, 0.4484, 0.4463, 0.4749, 0.4427,\n",
      "         0.4518],\n",
      "        [0.4814, 0.5329, 0.4511, 0.4874, 0.6104, 0.4421, 0.4477, 0.4827, 0.4516,\n",
      "         0.4567],\n",
      "        [0.4718, 0.5312, 0.4451, 0.4824, 0.6118, 0.4397, 0.4421, 0.4820, 0.4373,\n",
      "         0.4457],\n",
      "        [0.4659, 0.5303, 0.4475, 0.4845, 0.6092, 0.4441, 0.4444, 0.4795, 0.4340,\n",
      "         0.4422],\n",
      "        [0.4755, 0.5280, 0.4419, 0.4888, 0.6031, 0.4495, 0.4477, 0.4800, 0.4436,\n",
      "         0.4532],\n",
      "        [0.4697, 0.5352, 0.4445, 0.4887, 0.6061, 0.4433, 0.4444, 0.4790, 0.4455,\n",
      "         0.4529],\n",
      "        [0.4725, 0.5334, 0.4452, 0.4867, 0.6018, 0.4448, 0.4433, 0.4810, 0.4390,\n",
      "         0.4486],\n",
      "        [0.4793, 0.5274, 0.4392, 0.4855, 0.6035, 0.4444, 0.4450, 0.4777, 0.4391,\n",
      "         0.4557],\n",
      "        [0.4756, 0.5231, 0.4422, 0.4858, 0.6119, 0.4480, 0.4511, 0.4819, 0.4403,\n",
      "         0.4512],\n",
      "        [0.4718, 0.5376, 0.4423, 0.4904, 0.6098, 0.4387, 0.4443, 0.4916, 0.4422,\n",
      "         0.4483],\n",
      "        [0.4695, 0.5298, 0.4519, 0.4850, 0.6063, 0.4514, 0.4437, 0.4809, 0.4403,\n",
      "         0.4470],\n",
      "        [0.4773, 0.5311, 0.4515, 0.4846, 0.6074, 0.4442, 0.4466, 0.4789, 0.4428,\n",
      "         0.4521],\n",
      "        [0.4779, 0.5338, 0.4488, 0.4873, 0.6091, 0.4396, 0.4470, 0.4844, 0.4423,\n",
      "         0.4515],\n",
      "        [0.4719, 0.5347, 0.4502, 0.4861, 0.6116, 0.4405, 0.4453, 0.4884, 0.4400,\n",
      "         0.4439],\n",
      "        [0.4721, 0.5288, 0.4482, 0.4883, 0.6127, 0.4466, 0.4507, 0.4818, 0.4438,\n",
      "         0.4506],\n",
      "        [0.4791, 0.5304, 0.4434, 0.4873, 0.6094, 0.4409, 0.4469, 0.4811, 0.4442,\n",
      "         0.4550],\n",
      "        [0.4739, 0.5272, 0.4427, 0.4880, 0.6110, 0.4467, 0.4510, 0.4869, 0.4407,\n",
      "         0.4480],\n",
      "        [0.4720, 0.5306, 0.4422, 0.4893, 0.6083, 0.4463, 0.4476, 0.4817, 0.4468,\n",
      "         0.4544],\n",
      "        [0.4722, 0.5330, 0.4381, 0.4894, 0.6052, 0.4446, 0.4435, 0.4833, 0.4453,\n",
      "         0.4542],\n",
      "        [0.4734, 0.5346, 0.4459, 0.4856, 0.6069, 0.4397, 0.4434, 0.4799, 0.4418,\n",
      "         0.4500],\n",
      "        [0.4713, 0.5299, 0.4458, 0.4863, 0.6055, 0.4455, 0.4481, 0.4783, 0.4373,\n",
      "         0.4474],\n",
      "        [0.4740, 0.5315, 0.4429, 0.4875, 0.6081, 0.4420, 0.4460, 0.4817, 0.4442,\n",
      "         0.4500],\n",
      "        [0.4757, 0.5265, 0.4411, 0.4853, 0.6028, 0.4469, 0.4453, 0.4767, 0.4383,\n",
      "         0.4509],\n",
      "        [0.4666, 0.5344, 0.4481, 0.4883, 0.6111, 0.4441, 0.4452, 0.4857, 0.4439,\n",
      "         0.4443],\n",
      "        [0.4740, 0.5283, 0.4458, 0.4839, 0.6046, 0.4420, 0.4445, 0.4769, 0.4338,\n",
      "         0.4467],\n",
      "        [0.4718, 0.5297, 0.4461, 0.4858, 0.6058, 0.4455, 0.4432, 0.4791, 0.4367,\n",
      "         0.4498],\n",
      "        [0.4727, 0.5258, 0.4426, 0.4846, 0.6102, 0.4463, 0.4465, 0.4767, 0.4391,\n",
      "         0.4522],\n",
      "        [0.4694, 0.5259, 0.4467, 0.4850, 0.6102, 0.4458, 0.4492, 0.4806, 0.4322,\n",
      "         0.4437],\n",
      "        [0.4792, 0.5283, 0.4426, 0.4852, 0.6081, 0.4416, 0.4456, 0.4782, 0.4428,\n",
      "         0.4533],\n",
      "        [0.4745, 0.5295, 0.4501, 0.4881, 0.6070, 0.4492, 0.4464, 0.4809, 0.4448,\n",
      "         0.4534],\n",
      "        [0.4721, 0.5283, 0.4496, 0.4847, 0.6120, 0.4455, 0.4471, 0.4818, 0.4406,\n",
      "         0.4486],\n",
      "        [0.4769, 0.5323, 0.4413, 0.4889, 0.6088, 0.4429, 0.4467, 0.4828, 0.4511,\n",
      "         0.4559],\n",
      "        [0.4724, 0.5265, 0.4453, 0.4861, 0.6144, 0.4440, 0.4486, 0.4805, 0.4407,\n",
      "         0.4498],\n",
      "        [0.4765, 0.5292, 0.4450, 0.4860, 0.6099, 0.4438, 0.4473, 0.4803, 0.4454,\n",
      "         0.4537],\n",
      "        [0.4705, 0.5256, 0.4455, 0.4889, 0.6150, 0.4481, 0.4527, 0.4848, 0.4436,\n",
      "         0.4477],\n",
      "        [0.4745, 0.5328, 0.4492, 0.4871, 0.6113, 0.4410, 0.4465, 0.4812, 0.4468,\n",
      "         0.4519],\n",
      "        [0.4778, 0.5281, 0.4472, 0.4894, 0.6110, 0.4462, 0.4492, 0.4841, 0.4463,\n",
      "         0.4541],\n",
      "        [0.4776, 0.5301, 0.4501, 0.4856, 0.6076, 0.4431, 0.4474, 0.4817, 0.4367,\n",
      "         0.4490],\n",
      "        [0.4754, 0.5301, 0.4476, 0.4845, 0.6048, 0.4452, 0.4445, 0.4798, 0.4372,\n",
      "         0.4497],\n",
      "        [0.4730, 0.5296, 0.4493, 0.4882, 0.6112, 0.4480, 0.4482, 0.4876, 0.4428,\n",
      "         0.4474],\n",
      "        [0.4742, 0.5361, 0.4419, 0.4868, 0.6066, 0.4386, 0.4421, 0.4851, 0.4417,\n",
      "         0.4492],\n",
      "        [0.4823, 0.5293, 0.4427, 0.4863, 0.6065, 0.4412, 0.4492, 0.4813, 0.4405,\n",
      "         0.4540],\n",
      "        [0.4714, 0.5301, 0.4373, 0.4860, 0.6018, 0.4490, 0.4420, 0.4801, 0.4418,\n",
      "         0.4513],\n",
      "        [0.4773, 0.5339, 0.4426, 0.4914, 0.6076, 0.4406, 0.4499, 0.4813, 0.4513,\n",
      "         0.4579],\n",
      "        [0.4687, 0.5270, 0.4488, 0.4850, 0.6125, 0.4430, 0.4476, 0.4815, 0.4324,\n",
      "         0.4424],\n",
      "        [0.4733, 0.5318, 0.4503, 0.4849, 0.6110, 0.4418, 0.4450, 0.4874, 0.4359,\n",
      "         0.4431],\n",
      "        [0.4764, 0.5296, 0.4473, 0.4862, 0.6058, 0.4434, 0.4496, 0.4813, 0.4345,\n",
      "         0.4472],\n",
      "        [0.4741, 0.5331, 0.4514, 0.4883, 0.6060, 0.4478, 0.4427, 0.4845, 0.4473,\n",
      "         0.4514],\n",
      "        [0.4740, 0.5329, 0.4387, 0.4884, 0.6036, 0.4439, 0.4453, 0.4783, 0.4472,\n",
      "         0.4541],\n",
      "        [0.4759, 0.5331, 0.4578, 0.4870, 0.6065, 0.4478, 0.4455, 0.4829, 0.4457,\n",
      "         0.4502]])\n",
      "Attention weights: tensor([[0.0200, 0.0220, 0.0176,  ..., 0.0256, 0.0210, 0.0186],\n",
      "        [0.0255, 0.0250, 0.0158,  ..., 0.0214, 0.0241, 0.0137],\n",
      "        [0.0214, 0.0209, 0.0156,  ..., 0.0239, 0.0255, 0.0176],\n",
      "        ...,\n",
      "        [0.0214, 0.0207, 0.0176,  ..., 0.0237, 0.0223, 0.0148],\n",
      "        [0.0206, 0.0243, 0.0182,  ..., 0.0228, 0.0203, 0.0174],\n",
      "        [0.0232, 0.0238, 0.0163,  ..., 0.0247, 0.0240, 0.0143]])\n",
      "torch.Size([50, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        matmul_qk = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "        # scale matmul_qk\n",
    "        depth = query.shape[-1]\n",
    "        logits = matmul_qk / math.sqrt(depth)\n",
    "\n",
    "        # softmax is normalized on the last axis (seq_len_k)\n",
    "        attention_weights = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# Toy example\n",
    "query = torch.rand((50, 10))  # 50 queries, each of size 10\n",
    "key = torch.rand((50, 10))  # 50 keys, each of size 10\n",
    "value = torch.rand((50, 10))  # 50 values, each of size 10\n",
    "\n",
    "attention_layer = ScaledDotProductAttention()\n",
    "output, attention_weights = attention_layer(query, key, value)\n",
    "\n",
    "print(\"Output:\", output)\n",
    "print(\"Attention weights:\", attention_weights)\n",
    "\n",
    "print(attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.9434\n",
      "Epoch [2/15], Loss: 1.8854\n",
      "Epoch [3/15], Loss: 1.8049\n",
      "Epoch [4/15], Loss: 1.5884\n",
      "Epoch [5/15], Loss: 1.5126\n",
      "Epoch [6/15], Loss: 1.4796\n",
      "Epoch [7/15], Loss: 1.4721\n",
      "Epoch [8/15], Loss: 1.4669\n",
      "Epoch [9/15], Loss: 1.3825\n",
      "Epoch [10/15], Loss: 1.4160\n",
      "Epoch [11/15], Loss: 1.3881\n",
      "Epoch [12/15], Loss: 1.3914\n",
      "Epoch [13/15], Loss: 1.3567\n",
      "Epoch [14/15], Loss: 1.2998\n",
      "Epoch [15/15], Loss: 1.2995\n",
      "Accuracy of the model on the 10000 test inputs: 36.041666666666664 %\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 15\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "input_size =6\n",
    "num_classes = 3\n",
    "sequence_length = 3\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "heads = 2\n",
    "forward_expansion = 4\n",
    "\n",
    "# Create the model\n",
    "model = TransformerBlock(embed_size=input_size, heads=heads, dropout=dropout, forward_expansion=forward_expansion).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Toy dataset\n",
    "inputs = torch.randn(num_epochs * batch_size, sequence_length, input_size).to(device)\n",
    "labels = torch.randint(num_classes, (num_epochs * batch_size,)).to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, inputs.size(0), batch_size):\n",
    "        # Get mini-batch inputs and labels\n",
    "        inputs_mini = inputs[i:i+batch_size]\n",
    "        labels_mini = labels[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs_mini, inputs_mini, inputs_mini, mask=None)\n",
    "        outputs = outputs.mean(dim=1)\n",
    "        loss = criterion(outputs, labels_mini)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(0, inputs.size(0), batch_size):\n",
    "        inputs_mini = inputs[i:i+batch_size]\n",
    "        labels_mini = labels[i:i+batch_size]\n",
    "        outputs = model(inputs_mini, inputs_mini, inputs_mini, mask=None)\n",
    "        outputs = outputs.mean(dim=1)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels_mini.size(0)\n",
    "        correct += (predicted == labels_mini).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test inputs: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
