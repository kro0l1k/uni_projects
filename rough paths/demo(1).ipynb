{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://github.com/anh-tong/signax/tree/main/examples\n",
    "# https://docs.kidger.site/diffrax/examples/neural_cde/\n",
    "\n",
    "import math\n",
    "import jax\n",
    "import time\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.nn as jnn\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "from signax.module import SignatureTransform\n",
    "from functools import partial\n",
    "from fbm import FBM\n",
    "from jax import lax\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "import optax\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from sigkerax.sigkernel import SigKernel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Signatures for predicting Hurst exponent in fBM paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    # taken from equinox documentation\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jr.permutation(key, indices)\n",
    "        (key,) = jr.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(key, n_paths_train, n_paths_test, n_steps):\n",
    "    train_key, test_key = jr.split(key)\n",
    "    X_train, Y_train = [], []\n",
    "    X_test, Y_test = [], []\n",
    "    # generate train\n",
    "    hurst_exponents_train = jr.uniform(train_key, shape=(n_paths_train,), dtype=float, minval=0.2, maxval=0.8)\n",
    "    for hurst in hurst_exponents_train:\n",
    "        f = FBM(n_steps, float(hurst))\n",
    "        X_train.append(jnp.array(f.fbm()))\n",
    "\n",
    "    # generate test\n",
    "    hurst_exponents_test = jr.uniform(test_key, shape=(n_paths_test,), dtype=float, minval=0.2, maxval=0.8)   \n",
    "    for hurst in hurst_exponents_test:\n",
    "        f = FBM(n_steps, float(hurst))\n",
    "        X_test.append(jnp.array(f.fbm()))\n",
    "\n",
    "    X_train = jnp.array(X_train)\n",
    "    Y_train = hurst_exponents_train\n",
    "    X_test = jnp.array(X_test)\n",
    "    Y_test = hurst_exponents_test\n",
    "\n",
    "    return (\n",
    "        X_train[..., None],\n",
    "        Y_train[..., None],\n",
    "        X_test[..., None],\n",
    "        Y_test[..., None],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "key = jr.PRNGKey(seed)\n",
    "data_key, loader_key, model_key = jr.split(key, 3)\n",
    "\n",
    "n_paths_train = 1000\n",
    "n_paths_test = 50\n",
    "n_steps = 99\n",
    "X_train, Y_train, X_test, Y_test = generate_data(data_key, n_paths_train, n_paths_test, n_steps)\n",
    "\n",
    "iter_data = dataloader((X_train, Y_train), batch_size=128, key=loader_key)\n",
    "optim = optax.adam(learning_rate=1e-3)\n",
    "\n",
    "@eqx.filter_value_and_grad\n",
    "def compute_loss(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    assert pred_y.shape[0] == y.shape[0]\n",
    "    return jnp.mean(jnp.square(pred_y - y))\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, x, y, opt_state):\n",
    "    loss, grads = compute_loss(model, x, y)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return loss, model, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_convs(input_size: int, layer_sizes, kernel_size, *, key):\n",
    "    \"\"\"Make a stack of Conv1d:\n",
    "\n",
    "    The first layer has kernel size = `kernel_size`.\n",
    "    The remaining layers has kernel size = 1\n",
    "    \"\"\"\n",
    "    keys = jr.split(key, num=len(layer_sizes))\n",
    "    convs = []\n",
    "    first_conv = eqx.nn.Conv1d(\n",
    "        in_channels=input_size,\n",
    "        out_channels=layer_sizes[0],\n",
    "        kernel_size=kernel_size,\n",
    "        key=keys[0],\n",
    "    )\n",
    "    convs += [first_conv]\n",
    "    last_conv_size = layer_sizes[0]\n",
    "    for i, layer_size in enumerate(layer_sizes[1:]):\n",
    "        conv = eqx.nn.Conv1d(\n",
    "            in_channels=last_conv_size,\n",
    "            out_channels=layer_size,\n",
    "            kernel_size=1,\n",
    "            key=keys[i + 1],\n",
    "        )\n",
    "        convs += [conv]\n",
    "        last_conv_size = layer_size\n",
    "    return convs\n",
    "\n",
    "\n",
    "class Augment(eqx.nn.Sequential):\n",
    "    \"\"\"A stack of Conv1D, first Conv1D has kernel_size as input\n",
    "    The remaining Conv1D has kernel_size = 1\n",
    "\n",
    "    This allows to add original input and time dimension to the output\n",
    "    \"\"\"\n",
    "\n",
    "    activation: Callable\n",
    "    include_original: bool\n",
    "    include_time: bool\n",
    "    kernel_size: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers,\n",
    "        include_original=True,\n",
    "        include_time=True,\n",
    "        kernel_size=3,\n",
    "        activation=jax.nn.relu,\n",
    "    ):\n",
    "        self.layers = layers\n",
    "        self.include_original = include_original\n",
    "        self.include_time = include_time\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        *,\n",
    "        key=None,\n",
    "    ):\n",
    "        \"\"\"x size (length, dim)\"\"\"\n",
    "        length, _ = x.shape\n",
    "        ret = []\n",
    "        if self.include_original:\n",
    "            start_index = self.kernel_size - 1\n",
    "            truncated_x = x[start_index:]\n",
    "            ret.append(truncated_x)\n",
    "        if self.include_time:\n",
    "            time = jnp.linspace(0, 1, length - self.kernel_size + 1)\n",
    "            time = time[:, None]\n",
    "            ret.append(time)\n",
    "        augmented_x = self.layers[0](x.transpose())\n",
    "        for layer in self.layers[1:]:\n",
    "            augmented_x = self.activation(augmented_x)\n",
    "            augmented_x = layer(augmented_x)\n",
    "        ret.append(augmented_x.transpose())\n",
    "        return jnp.concatenate(ret, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "def signature_dim(n_channels, depth):\n",
    "    return sum([n_channels ** (i + 1) for i in range(depth)])\n",
    "\n",
    "\n",
    "def create_simple_net(\n",
    "    dim=1,\n",
    "    signature_depth=4,\n",
    "    augment_layer_size=(8, 8, 2),\n",
    "    augmented_kernel_size=1,\n",
    "    augmented_include_original=True,\n",
    "    augmented_include_time=True,\n",
    "    mlp_width=32,\n",
    "    mlp_depth=2,\n",
    "    output_size=1,\n",
    "    final_activation=jax.nn.sigmoid,\n",
    "    *,\n",
    "    key,\n",
    "):\n",
    "    augment_key, mlp_key = jr.split(key)\n",
    "\n",
    "    # create Convolutional augmented layers\n",
    "    convs = _make_convs(\n",
    "        input_size=dim,\n",
    "        layer_sizes=augment_layer_size,\n",
    "        kernel_size=augmented_kernel_size,\n",
    "        key=augment_key,\n",
    "    )\n",
    "    augment = Augment(\n",
    "        layers=convs,\n",
    "        include_original=augmented_include_original,\n",
    "        include_time=augmented_include_time,\n",
    "        kernel_size=augmented_kernel_size,\n",
    "    )\n",
    "\n",
    "    signature = SignatureTransform(depth=signature_depth)\n",
    "\n",
    "    # calculate output dimension of Agument\n",
    "    last_dim = augment_layer_size[-1]\n",
    "    if augmented_include_original:\n",
    "        last_dim += dim\n",
    "    if augmented_include_time:\n",
    "        last_dim += 1\n",
    "    # the output dimension of signature\n",
    "    mlp_input_dim = signature_dim(n_channels=last_dim, depth=signature_depth)\n",
    "    mlp = eqx.nn.MLP(\n",
    "        in_size=mlp_input_dim,\n",
    "        width_size=mlp_width,\n",
    "        depth=mlp_depth,\n",
    "        out_size=output_size,\n",
    "        final_activation=final_activation,\n",
    "        key=mlp_key,\n",
    "    )\n",
    "    layers = [augment, signature, mlp]\n",
    "\n",
    "    return eqx.nn.Sequential(layers=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_simple_net(\n",
    "    dim=1,\n",
    "    signature_depth=3,\n",
    "    augment_layer_size=(3,),\n",
    "    augmented_kernel_size=3,\n",
    "    mlp_width=32,\n",
    "    mlp_depth=5,\n",
    "    output_size=1,\n",
    "    final_activation=jax.nn.sigmoid,\n",
    "    key=model_key,\n",
    ")\n",
    "\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "test_mse = []\n",
    "for step, (x, y) in zip(range(500), iter_data):\n",
    "    loss, model, opt_state = make_step(model, x, y, opt_state)\n",
    "    loss = loss.item()\n",
    "    test_mse += [jnp.mean(jnp.square(jax.vmap(model)(X_test) - Y_test)).item()]\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step={step} \\t loss={loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_mse)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(x, y):\n",
    "    return scipy.stats.pearsonr(x, y)[0] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=jax.vmap(model)(X_test), y=Y_test)\n",
    "r2(jax.vmap(model)(X_test)[:,0], Y_test[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signature kernel SVM for time series classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ArticularyWordRecognition' # http://www.timeseriesclassification.com/description.php?Dataset=ArticularyWordRecognition\n",
    "\n",
    "x_train, y_train, x_test, y_test = UCR_UEA_datasets(use_cache=True).load_dataset(dataset)\n",
    "\n",
    "# normalize input paths\n",
    "x_train /= x_train.max()\n",
    "x_test /= x_test.max()\n",
    "\n",
    "x_train = 0.1 * x_train[:,::3,:]\n",
    "x_test = 0.1 * x_test[:,::3,:]\n",
    "\n",
    "# encode outputs as labels\n",
    "y_train = LabelEncoder().fit_transform(y_train)\n",
    "y_test = LabelEncoder().fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1,x_train.shape[-1]):\n",
    "    plt.plot(x_train[0][:,k])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy -> jax\n",
    "x_train = jnp.array(x_train)\n",
    "x_test = jnp.array(x_test)\n",
    "\n",
    "# define grid-search hyperparameters for SVC\n",
    "svc_parameters = {'C': np.logspace(0, 4, 5), 'gamma': list(np.logspace(-4, 4, 9)) + ['auto']}\n",
    "\n",
    "# define grid search hyperparameters for the RBF-signature kernel\n",
    "_sigmas = [1e-3, 5e-3, 1e-2, 2.5e-2, 5e-2, 7.5e-2, 1e-1, 2.5e-1, 5e-1, 7.5e-1, 1., 2., 5., 10.]\n",
    "\n",
    "# grid search over sigmas\n",
    "for sigma in _sigmas:\n",
    "    \n",
    "    # initialize signature PDE kernel\n",
    "    signature_kernel = SigKernel(refinement_factor=1, static_kernel_kind=\"rbf\", scales = jnp.array([sigma]), add_time=False)\n",
    "\n",
    "    # compute Gram matrix on train data\n",
    "    G_train = signature_kernel.kernel_matrix(x_train, x_train)[...,0]\n",
    "    \n",
    "    # SVC sklearn estimator\n",
    "    svc = SVC(kernel='precomputed', decision_function_shape='ovo')\n",
    "    svc_model = GridSearchCV(estimator=svc, param_grid=svc_parameters, cv=5, n_jobs=-1)\n",
    "    svc_model.fit(G_train, y_train)\n",
    "    \n",
    "    # compute Gram matrix on test data\n",
    "    G_test = signature_kernel.kernel_matrix(x_test, x_train)[...,0]\n",
    "    \n",
    "    # record scores\n",
    "    train_score = svc_model.best_score_\n",
    "    test_score = svc_model.score(G_test, y_test)\n",
    "    print(f\"RBF-signature kernel, sigma: {sigma},\", \n",
    "          f'training accuracy: {round(100*train_score,3)} %,', \n",
    "          f'testing accuracy: {round(100*test_score,3)} %. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural CDEs for time series classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, add_noise, *, key):\n",
    "    theta_key, noise_key = jr.split(key, 2)\n",
    "    length = 100\n",
    "    theta = jr.uniform(theta_key, (dataset_size,), minval=0, maxval=2 * math.pi)\n",
    "    y0 = jnp.stack([jnp.cos(theta), jnp.sin(theta)], axis=-1)\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0, 4 * math.pi, length), (dataset_size, length))\n",
    "    matrix = jnp.array([[-0.3, 2], [-2, -0.3]])\n",
    "    ys = jax.vmap(\n",
    "        lambda y0i, ti: jax.vmap(lambda tij: jsp.linalg.expm(tij * matrix) @ y0i)(ti)\n",
    "    )(y0, ts)\n",
    "    ys = jnp.concatenate([ts[:, :, None], ys], axis=-1)  # time is a channel\n",
    "    ys = ys.at[: dataset_size // 2, :, 1].multiply(-1)\n",
    "    if add_noise:\n",
    "        ys = ys + jr.normal(noise_key, ys.shape) * 0.1\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = jnp.zeros((dataset_size,))\n",
    "    labels = labels.at[: dataset_size // 2].set(1.0)\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size,\n",
    "            out_size=hidden_size * data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.softplus,\n",
    "            # Note the use of a tanh final activation function. This is important to\n",
    "            # stop the model blowing up. (Just like how GRUs and LSTMs constrain the\n",
    "            # rate of change of their hidden states.)\n",
    "            final_activation=jnn.tanh,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.mlp(y).reshape(self.hidden_size, self.data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey = jr.split(key, 3)\n",
    "        self.initial = eqx.nn.MLP(data_size, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(data_size, hidden_size, width_size, depth, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 1, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs)\n",
    "        term = diffrax.ControlTerm(self.func, control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jnn.sigmoid(self.linear(solution.ys[-1]))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size=256\n",
    "add_noise=False\n",
    "batch_size=32\n",
    "lr=1e-3\n",
    "steps=50\n",
    "hidden_size=8\n",
    "width_size=128\n",
    "depth=1\n",
    "seed=5678\n",
    "\n",
    "key = jr.PRNGKey(seed)\n",
    "train_data_key, test_data_key, model_key, loader_key = jr.split(key, 4)\n",
    "\n",
    "ts, coeffs, labels, data_size = get_data(dataset_size, add_noise, key=train_data_key)\n",
    "\n",
    "model = NeuralCDE(data_size, hidden_size, width_size, depth, key=model_key)\n",
    "\n",
    "# Training loop like normal.\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss(model, ti, label_i, coeff_i):\n",
    "    pred = jax.vmap(model)(ti, coeff_i)\n",
    "    # Binary cross-entropy\n",
    "    bxe = label_i * jnp.log(pred) + (1 - label_i) * jnp.log(1 - pred)\n",
    "    bxe = -jnp.mean(bxe)\n",
    "    acc = jnp.mean((pred > 0.5) == (label_i == 1))\n",
    "    return bxe, acc\n",
    "\n",
    "grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, data_i, opt_state):\n",
    "    ti, label_i, *coeff_i = data_i\n",
    "    (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return bxe, acc, model, opt_state\n",
    "\n",
    "optim = optax.adam(lr)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "for step, data_i in zip(\n",
    "    range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "):\n",
    "    start = time.time()\n",
    "    bxe, acc, model, opt_state = make_step(model, data_i, opt_state)\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "        f\"{end - start}\"\n",
    "    )\n",
    "\n",
    "ts, coeffs, labels, _ = get_data(dataset_size, add_noise, key=test_data_key)\n",
    "bxe, acc = loss(model, ts, labels, coeffs)\n",
    "print(f\"Test loss: {bxe}, Test Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
